# Will create a cluster with 1 master + 3 worker nodes, install LINSTOR and run the tests.
# The following components will be scheduled on the master node, so that they are "highly available":
# * etcd
# * LINSTOR Controller
# * LINSTOR Operator
# * CSI Controller
# * CSI Snapshot Controller
# Expects 4 VMs provisioned by ./vms/e2e.provision.toml
[values]
KubernetesVersion = "1.21.1"
KubernetesCNIURL = "https://docs.projectcalico.org/manifests/calico.yaml"
KubeadmPlaybookVersion = "v1.20"
LinstorCSIImage = "drbd.io/linstor-csi:latest"
LinstorChartValues = """
.masteraffinity: &masteraffinity
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchExpressions:
        - key: node-role.kubernetes.io/master
          operator: Exists
.mastertoleration: &mastertoleration
  - key: node-role.kubernetes.io/master
    operator: Exists
    effect: NoSchedule

stork:
  enabled: false
csi:
  controllerAffinity: *masteraffinity
  controllerTolerations: *mastertoleration
  enableTopology: true
csi-snapshotter:
  affinity: *masteraffinity
  tolerations: *mastertoleration
operator:
  affinity: *masteraffinity
  tolerations: *mastertoleration
  controller:
    affinity: *masteraffinity
    tolerations: *mastertoleration
  satelliteSet:
    storagePools:
      lvmThinPools:
        - name: e2epool
          volumeGroup: ""
          thinVolume: thin
          devicePaths:
          - /dev/vdb
"""
ChartRepoURL = "https://charts.linstor.io"
DockerServer = "drbd.io"
DockerUsername = "<fill>"
DockerPassword = "<fill>"

[[steps]]
[steps.docker]
image = "alpine"
command = ["sh", "-ec", """
apk add ansible curl openssh-client

mkdir /kubeadm-playbook
curl -L "https://github.com/ReSearchITEng/kubeadm-playbook/archive/{{ .KubeadmPlaybookVersion }}.tar.gz" | tar xz --strip-components=1 -C /kubeadm-playbook
cd /kubeadm-playbook

echo "===== Prepare SSH access"

IFS=,; for t in $TARGETS; do
	if [ "$master_host" = "" ]; then
		master_host=$t
	elif [ "$other_hosts" = "" ]; then
		other_hosts=$t
	else
		other_hosts="$other_hosts\n$t"
	fi
done
IFS=" \t\n"

echo "==== Generate Ansible inventory"

cat <<-EOF > hosts.ini
[primary-master]
$master_host

[secondary-masters]

[masters:children]
primary-master
secondary-masters

[nodes]
$other_hosts
EOF

echo "=== ansible inventory:"
cat hosts.ini
echo "=== ansible inventory end"
echo "=== extra vars:"
echo "$EXTRA_VARS"
echo "=== extra vars end"
echo "===== Install with kubeadm-playbook"
ansible-playbook all_install.yml -i hosts.ini -e "$EXTRA_VARS" --skip-tags common_install,prepull_images,storage,helm

echo "=== set up mutual SSH access"

cat <<-EOF > ssh_access.yml
---
- name: Ensure mutual ssh access
  hosts: all
  tasks:
  - name: create .ssh directory
    file:
      path: /root/.ssh
      state: directory
      mode: '0755'
  - name: copy ssh key
    copy:
      src: /root/.ssh/id_rsa
      dest: /root/.ssh/id_rsa
      mode: '0600'
  - name: copy known_hosts
    copy:
      src: /root/.ssh/known_hosts
      dest: /root/.ssh/known_hosts
      mode: '0644'
EOF
ansible-playbook ssh_access.yml -i hosts.ini

echo "=== configure kubectl and helm"
curl -fsSL https://get.helm.sh/helm-v3.4.1-linux-amd64.tar.gz | tar --strip-components=1 -xzC /usr/local/bin/ linux-amd64/helm
curl -fsSL "https://storage.googleapis.com/kubernetes-release/release/v{{ .KubernetesVersion }}/bin/linux/amd64/kubectl" > /usr/local/bin/kubectl
chmod +x /usr/local/bin/kubectl

mkdir -p ~/.kube
scp $master_host:/etc/kubernetes/admin.conf ~/.kube/config

echo "=== install linstor on k8s"

MASTER_NODE="$(kubectl get nodes -l node-role.kubernetes.io/master -o jsonpath='{.items[*].metadata.name}')"
helm repo add linstor $CHART_REPO_URL
helm install linstor-etcd-pv linstor/pv-hostpath --set "nodes={$MASTER_NODE}"

cat <<-EOF > helm-values.yaml
{{ .LinstorChartValues }}
EOF

echo "=== helm override:"
cat helm-values.yaml
echo "=== helm override end"

kubectl create namespace linstor-csi-e2e
kubectl create --namespace linstor-csi-e2e secret docker-registry drbdiocred --docker-server=${DOCKER_SERVER} --docker-username=${DOCKER_USERNAME} --docker-password=${DOCKER_PASSWORD}

helm install linstor linstor/linstor --values helm-values.yaml --namespace linstor-csi-e2e --set csi.pluginImage=${PLUGIN_IMAGE}

kubectl --namespace linstor-csi-e2e wait --for=condition=Available --timeout=5m deployments/linstor-operator

# wait for the operator to create more pods
sleep 10
kubectl --namespace linstor-csi-e2e wait --for=condition=Ready --timeout=10m pod --all

# Apply default storage class for tests
cat <<-EOF > default-sc.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: default-linstor-sc
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
provisioner: linstor.csi.linbit.com
allowVolumeExpansion: true
parameters:
  autoPlace: "2"
  storagePool: "e2epool"
  resourceGroup: "default-linstor-sc"
  csi.storage.k8s.io/fstype: xfs
EOF
kubectl apply -f default-sc.yaml

echo "linstor on k8s ready"
"""]

[steps.docker.env]
EXTRA_VARS = """
{
    "KUBERNETES_VERSION": "{{.KubernetesVersion}}",
    "k8s_network_addons_urls": ["{{.KubernetesCNIURL}}"],
}
"""
CHART_REPO_URL = "{{ .ChartRepoURL }}"
PLUGIN_IMAGE = "{{ .LinstorCSIImage }}"
DOCKER_SERVER = "{{ .DockerServer }}"
DOCKER_USERNAME = "{{ .DockerUsername }}"
DOCKER_PASSWORD = "{{ .DockerPassword }}"

[[steps]]
[steps.rsync]
source = "./test/bin/e2e"
dest = "/opt/teste2e"

[[steps]]
[steps.shell]
script = """
set -x
# Only run once on master
if ! kubectl get nodes -l node-role.kubernetes.io/master | grep "$(hostname -f)" >/dev/null ; then
    exit 0
fi

/opt/teste2e --ginkgo.reportFile=/e2e-report.xml --linstor-csi-e2e.storage-pool=e2epool --linstor-csi-e2e.volume-replicas=2

echo "===== Collect cluster logs ====="
mkdir -p /logs
kubectl get pods --namespace linstor-csi-e2e -o name | xargs -i sh -c 'kubectl logs --namespace linstor-csi-e2e "{}" --all-containers --prefix > "/logs/$(basename {}).log"'
"""
